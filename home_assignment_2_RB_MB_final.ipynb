{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social Data Science WS19/20\n",
    "\n",
    "# Home Assignment 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit your solution via Moodle until 23.59pm on Wednesday, November 27th. Late submissions are accepted for 12 hours following the deadline, with 1/3 of the total possible points deducted from the score.\n",
    "\n",
    "Submit your solutions in teams of up 2-3 members. **Single student submissions will not be graded anymore.**\n",
    "Please denote all members of the team with their student id and full name in the notebook. In this home assignment, you have to submit an .ipynb notebook and a file \"beta.npy\" as specified in task 3. Do not submit anything else than these two files!\n",
    "\n",
    "Cite ALL your sources for coding this home assignment. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members will be expelled from the course without warning.\n",
    "\n",
    "If you have any general questions regarding this assignment, please ask on Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List team members, including all student IDs here:\n",
    "1. Romayssa\n",
    "2. Moncef=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Causal Inference (1 pt)\n",
    "\n",
    "Suppose you conduct a study to evaluate the effect of a new procedure for coronary bypass surgery that is supposed to help with the postoperative healing process. Since the new procedure is more risky than the old one, it is rarely performed on patients who are over 80 years old. However, there is also hardly any data on under 80-year olds that have taken the old treatment. \n",
    "\n",
    "You can find the data from the study in \"bypass.csv\", where _stay_ is the length of the hospital stay after surgery, _age_ is the age of the patient, and _new_ is the binary indicator variable specifying whether the new surgical procedure was used. Additionally, there is a column _severity_ which quantifies the severity of preoperative diseases.\n",
    "\n",
    "Perform a regression analysis to draw a causal inference whether the new surgery method shortens the length of the postoperative hospital stay. Can you observe a significant effect? Would you generally recommend to apply the new surgery method?\n",
    "Provide a thorough explanation of your answers and the steps in your analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <center> **Steps of the analysis** </center>\n",
    "*The goal is to evaluate the effect of the new surgery. For this we want to prove that the difference of the stay period for an individual under the new surgery method is really different from the stay period under the old surgery method.\n",
    "As a matter of fact we want to proove that the new surgery method is in CAUSE (inference of causality) of the changement of the stay period.*\n",
    "\n",
    "</br>\n",
    "As mentionned there are few problems with our Data:\n",
    "\n",
    "1.   More individulas with the new surgery method than the old one (168 vs 32 enteries).\n",
    "2.   Age is unbalanced: much more individuals with new surgery method are younger than 80 and much more individuals are older than 80 with the old surgery.\n",
    "3. An individual is also characterized by his age and also the severity of his health. \n",
    "\n",
    "These problems make the study of correlation between the **surgery method** and the **stay** period and the causality inference ambiguous and not effective.\n",
    "</br>\n",
    "\n",
    "</br>\n",
    "\n",
    "### Step 1: Matching Data\n",
    "To tackle these probelms we will use a method seen in the \n",
    "\n",
    "lecture, the **Matching Method**:\n",
    "\n",
    "- We will devide our data into two groups: old method data points and new method data points. \n",
    "\n",
    "- We will apply a matching method by forming two groups that are similar by age and also severity and that had new method or old method surgery.\n",
    "\n",
    "- We will generate a new Dataset with this matched Data that can be analyzed.\n",
    "\n",
    "\n",
    "### Step 2: Hypothesis test\n",
    "\n",
    "- null hypothesis:\n",
    "\n",
    "\n",
    "    There is a no difference in stay between the old surgery method and the new surgery method.\n",
    "- alternative hypothesis:\n",
    "\n",
    "\n",
    "    The new surgery method decrease significatly the stay period.\n",
    "\n",
    "</br>\n",
    "\n",
    "#### How to perform this test hypothesis ?\n",
    "\n",
    "- With this new Dataset, we will compare the mean of the stay period for the two groups (new method and old method).\n",
    "- We will also do a multi-attribute regression inference.to see if the surgery method correlate with the stay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the file\n",
    "data= pd.read_csv(\"bypass.csv\") #load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 336 entries, 0 to 167\n",
      "Data columns (total 4 columns):\n",
      "age         336 non-null float64\n",
      "severity    336 non-null float64\n",
      "new         336 non-null int64\n",
      "stay        336 non-null float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 13.1 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>severity</th>\n",
       "      <th>new</th>\n",
       "      <th>stay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>336.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>336.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>73.151058</td>\n",
       "      <td>49.382448</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>27.779566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.246904</td>\n",
       "      <td>9.128871</td>\n",
       "      <td>0.500746</td>\n",
       "      <td>4.465878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>50.637442</td>\n",
       "      <td>26.687629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.357648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>68.977574</td>\n",
       "      <td>41.164295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.376341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>76.224462</td>\n",
       "      <td>47.904978</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>28.447573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>78.797649</td>\n",
       "      <td>56.915571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.881043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>83.580951</td>\n",
       "      <td>72.054950</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>36.750665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age    severity         new        stay\n",
       "count  336.000000  336.000000  336.000000  336.000000\n",
       "mean    73.151058   49.382448    0.500000   27.779566\n",
       "std      7.246904    9.128871    0.500746    4.465878\n",
       "min     50.637442   26.687629    0.000000   15.357648\n",
       "25%     68.977574   41.164295    0.000000   24.376341\n",
       "50%     76.224462   47.904978    0.500000   28.447573\n",
       "75%     78.797649   56.915571    1.000000   30.881043\n",
       "max     83.580951   72.054950    1.000000   36.750665"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## function from the instruction of Social DS. that do a greedy matching between two populations\n",
    "# We assume that group 1 is bigger than group 2\n",
    "def greedy_matching (group1,group2):\n",
    "    group_1 = group1.copy()\n",
    "    \n",
    "    # get inverse covariance matrix for mahalanobis distance\n",
    "    cov = np.cov(np.vstack([group1, group2]).T) # note that numpy's cov() function computes covariances between ROWS\n",
    "    invcov = np.linalg.inv(cov)\n",
    "    # for each element in the smaller group pick one greedily from the first group\n",
    "    matching = {}\n",
    "    for i,o2 in enumerate(group2):\n",
    "        #compute distances:\n",
    "        distances = [distance.mahalanobis(o2, o1, invcov) for o1 in group1]\n",
    "        #find the index of the best match for element o2\n",
    "        best_match = np.argmin(distances)\n",
    "        #store the matching\n",
    "        matching [i] = best_match\n",
    "        #prevent matching the same best_match again\n",
    "        group_1[best_match].fill(np.inf)     \n",
    "    return matching\n",
    "\n",
    "# specify columns to match on and both new and old surgery groups\n",
    "match_cols = ['age','severity']\n",
    "\n",
    "old_group = data.loc[data.new == 0, match_cols]\n",
    "new_group = data.loc[data.new == 1, match_cols]\n",
    "matching = greedy_matching(old_group.to_numpy(), new_group.to_numpy())\n",
    "matching\n",
    "\n",
    "# get resulting control group after matching\n",
    "all_cols = ['age','severity','new','stay']\n",
    "matched_old_group = data.loc[data.new==0,all_cols].reset_index(drop=True)\n",
    "matched_old_group = matched_old_group.iloc[list(matching.values())].reset_index(drop=True)\n",
    "matched_old_group\n",
    "matched_new_group = data.loc[data.new==1,all_cols].reset_index(drop=True)\n",
    "matched_new_group\n",
    "# stack matched data\n",
    "matched_data = matched_new_group.append(matched_old_group)\n",
    "matched_data.info()\n",
    "matched_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of stay with the new surgery method: 24.660080097356843\n",
      "The mean of stay with the old surgery method: 30.899051922951518\n"
     ]
    }
   ],
   "source": [
    "mean_new = np.mean(matched_data.loc[matched_data.new == 1,\"stay\"])\n",
    "mean_old = np.mean(matched_data.loc[matched_data.new == 0,\"stay\"])\n",
    "print(\"The mean of stay with the new surgery method:\",mean_new)\n",
    "print(\"The mean of stay with the old surgery method:\",mean_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joe\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>stay</td>       <th>  R-squared:         </th> <td>   0.996</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.996</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>3.051e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 27 Nov 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:27:55</td>     <th>  Log-Likelihood:    </th> <td> -34.426</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   336</td>      <th>  AIC:               </th> <td>   76.85</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   332</td>      <th>  BIC:               </th> <td>   92.12</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    0.7795</td> <td>    0.220</td> <td>    3.547</td> <td> 0.000</td> <td>    0.347</td> <td>    1.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>      <td>    0.2010</td> <td>    0.003</td> <td>   68.510</td> <td> 0.000</td> <td>    0.195</td> <td>    0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>new</th>      <td>   -4.6257</td> <td>    0.042</td> <td> -110.309</td> <td> 0.000</td> <td>   -4.708</td> <td>   -4.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>severity</th> <td>    0.2959</td> <td>    0.002</td> <td>  174.237</td> <td> 0.000</td> <td>    0.293</td> <td>    0.299</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.397</td> <th>  Durbin-Watson:     </th> <td>   2.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.820</td> <th>  Jarque-Bera (JB):  </th> <td>   0.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.049</td> <th>  Prob(JB):          </th> <td>   0.769</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.833</td> <th>  Cond. No.          </th> <td>1.34e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.34e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   stay   R-squared:                       0.996\n",
       "Model:                            OLS   Adj. R-squared:                  0.996\n",
       "Method:                 Least Squares   F-statistic:                 3.051e+04\n",
       "Date:                Wed, 27 Nov 2019   Prob (F-statistic):               0.00\n",
       "Time:                        20:27:55   Log-Likelihood:                -34.426\n",
       "No. Observations:                 336   AIC:                             76.85\n",
       "Df Residuals:                     332   BIC:                             92.12\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.7795      0.220      3.547      0.000       0.347       1.212\n",
       "age            0.2010      0.003     68.510      0.000       0.195       0.207\n",
       "new           -4.6257      0.042   -110.309      0.000      -4.708      -4.543\n",
       "severity       0.2959      0.002    174.237      0.000       0.293       0.299\n",
       "==============================================================================\n",
       "Omnibus:                        0.397   Durbin-Watson:                   2.234\n",
       "Prob(Omnibus):                  0.820   Jarque-Bera (JB):                0.525\n",
       "Skew:                          -0.049   Prob(JB):                        0.769\n",
       "Kurtosis:                       2.833   Cond. No.                     1.34e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.34e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We perform an Ordinary Least Squares Regression\n",
    "\n",
    "pred_cols = ['age','new','severity']\n",
    "X = matched_data[pred_cols]\n",
    "X= sm.add_constant(matched_data[pred_cols])\n",
    "y = matched_data.stay\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "\n",
    "We notice the difference in the means of period of stay between the two sugery methods is significant 24.6 against 30.89.\n",
    "\n",
    "The linear regression shows that the age and the sevirity do influence the period of healing but not as much as the new surgery method. The surgery method is highly correlated to the stay attribute ~ - 4.62. As our data is a matched data we can infere a causality relation between the new surgery and the stay period. \n",
    "\n",
    "Back to our hypothesis: We notice that the T-value is -110.309 and the P-value is null. (We have a negative influence). We refute the null hypothesis and accept the alternative: The new surgery method reduce the period of stay in the hospital.\n",
    "\n",
    "**As a result: We recommand the use of this new surgery method.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Naive Bayes on Continuous Data (1 pt)\n",
    "\n",
    "In lecture we have introduced the _Naive Bayes_ classifier, in which for each feature, we compute the empirical probabilities \n",
    "$ P(x_i|y) $ to form our predictions. For discrete data, we have learned in lecture that these probabilities are estimated from the raw class-conditional feature counts. For continuous features however, this approach obviously does not make sense.\n",
    "Instead, for continuous features, one usually makes the assumption that these  are normally distributed. Thus for continuous features, one usually models the class-conditional probabilities via \n",
    "$$ P(X_i = x| y = C_k) = \\frac{1}{\\sqrt{2\\pi \\sigma_{ik}^2}} \\exp\\left(-\\frac{(x_i-\\mu_{ik})^2}{2\\sigma_{ik}^2}\\right)$$\n",
    "where $\\mu_{ik}$ and $\\sigma_{ik}$ are the mean and variance of the features $X_i$ that are classed as $c_k$.\n",
    "A Naive Bayes classifier for which all features are assumed to be continuous and normally distribited is also called _Gaussian Naive Bayes_ classifier.\n",
    "\n",
    "Implement a class that performs Naive Bayes classification on both discrete and continuous data, using the class and function signatures and specifications in the cell below, which are designed to be similar to the model classes in sklearn. \n",
    "When passing your data to the fit() function, by default your classifier should treat all columns as continuous, i.e. normally distributed as explained above. Discrete features have to be specified upon fitting, by passing a list of integers specifying the indices of the columns that are to be treated as discrete features.\n",
    "\n",
    "Note: You may use any function from the numpy library, but __NO__ function from the scikit-learn library. Further, you may assume that all input data is valid, i.e. you do not need to check whether for instance the feature matrix X has as many rows as the class vector $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.dataset_classes = []\n",
    "        self.discrete_columns = []\n",
    "        self.probabilities = {}\n",
    "\n",
    "    # fit your model, nothing is returned\n",
    "    # X: two dimensional numpy array describing the feature matrix to train on\n",
    "    # y: one dimensional numpy array or list representing the class vector used in training\n",
    "    # discrete columns: optional list of indices describing the coumns in X that are to be treated as numerical.\n",
    "    def fit(self, X, y, discrete_columns=None):\n",
    "        self.dataset_classes = list(set(y))\n",
    "        self.discrete_columns = discrete_columns\n",
    "        X = pd.DataFrame(data=X, columns=None)\n",
    "        X[\"class_\"] = y\n",
    "        for i, col in enumerate(X.columns[:-1]):\n",
    "            if discrete_columns and i in discrete_columns:\n",
    "                # Processing a discrete column\n",
    "                for val in set(X.iloc[:, i]):\n",
    "                    for class_ in self.dataset_classes:\n",
    "                        self.probabilities[f\"{val}_{class_}\"] = X[(X[col] == val) & (X[\"class_\"] == class_)].shape[0] / \\\n",
    "                                                                X[(X[col] == val)].shape[0]\n",
    "            else:\n",
    "                # Processing continuous column\n",
    "                for class_ in self.dataset_classes:\n",
    "                    target = X[X[\"class_\"] == class_][col]\n",
    "                    variance = np.var(target)\n",
    "                    mean = np.mean(target)\n",
    "                    self.probabilities[f\"{col}_{class_}\"] = {'variance': variance, 'mean': mean}\n",
    "        for class_ in self.dataset_classes:\n",
    "            self.probabilities[class_] = X[(X[\"class_\"] == class_)].shape[0] / X[\"class_\"].shape[0]\n",
    "\n",
    "    def __proba_calc(self, row, class_):\n",
    "        proba = 1\n",
    "        for i, val in enumerate(row):\n",
    "            if self.discrete_columns and i in self.discrete_columns:\n",
    "                # Processing a discrete column\n",
    "                key = f\"{val}_{class_}\"\n",
    "                proba *= self.probabilities[key]\n",
    "            else:\n",
    "                # Processing continuous column\n",
    "                key = f\"{i}_{class_}\"\n",
    "                proba *= (1 / np.sqrt(2 * np.pi * self.probabilities[key][\"variance\"])) * np.exp(\n",
    "                    (-1) * np.power(val - self.probabilities[key][\"mean\"], 2) / (\n",
    "                                2 * self.probabilities[key][\"variance\"]))\n",
    "        return proba\n",
    "\n",
    "    # predict function: use the previously trained model to make a prediction\n",
    "    # -> return a vector y_hat of predicted classes.\n",
    "    # X: two dimensional numpy array describing the feature matrix to predict on\n",
    "    def predict(self, X):\n",
    "        y_hat = []\n",
    "        for row in X:\n",
    "            proba = []\n",
    "            for class_ in self.dataset_classes:\n",
    "                proba.append(self.__proba_calc(row, class_)*self.probabilities[class_])\n",
    "            y_hat.append(self.dataset_classes[np.argmax(proba)])\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Predicting House Prizes (3 pts)\n",
    "\n",
    "In this task you have to create a regression model to predict housing prices. We are providing you with a training dataset that you have to use to form your model, but we are going to grade your model based on the performance on a test set that is not known to you.\n",
    "\n",
    "__Grading:__ We will measure the performance of your model in terms of the MSE error on a test set that we are holding out. We will create a simple baseline model ourselves, which will be based on a little feature engineering, and a ridge regression model that is just run with default parameters from scikit-learn. To obtain 50% of the points in this task, your model should show a better performance on the test set than our baseline. The remaining 50% of the points will be awarded based on your performance against the rest of the class. The best models will get full marks, and the rest of the marks will degrade along with the testing error scores.\n",
    "\n",
    "\n",
    "__What to submit:__ In your submission, you have to provide the following things: \n",
    "1. Two functions that preprocess the input data into the format that you chose to use in your final regression model. The first function should perform the main part of the feature engineering and return an unscaled predictor matrix. Thus, its input has to be a pandas dataframe consisting of all the predictor colums in the data, and its output has to be a two-dimensional numpy array $X$. The second function should scale the preprocessed matrix. Its input is a training matrix that your scaling model is based on, and a test matrix that your scaling model is then to be applied on, to return a scaled test matrix. The exact signatures and the testing pipeline can be found below. Note that the scaling function may also perform other things than scaling, e.g. adding a constant column or adding interactions, as long as the testing pipeline below works properly. \n",
    "2. A corresponding parameter vector $\\beta$, that is saved as a numpy array into the file \"beta.npy\". It is used to compute the model predictions via $\\hat{y} = X\\beta$. Note that any other models next to regression models are not admissible.\n",
    "3. All the code that you used to optimize your final model. Submissions that show no code in that regard will not be awarded any points. Further, we encourage to report all the exploratory analysis that you did, and report some of the errors that your model achieved on some validation data. This may grant some extra points in the case that your model does not outperform our baseline on the testing data.\n",
    "\n",
    "Note that in this task, you may use any Python library that can be installed via pip. In particular, you may use libraries like statsmodel or scikit-learn. \n",
    "\n",
    "In the cells below, you can find the signatures of the preprocessing and scaling functions, the pipeline that we are going to use to make predictions, and some space where your code for exploration and optimization can go. Note that your model, i.e. both preprocessing functions and the parameter vector have to work on this pipeline. If this pipeline cell does not execute, you will not get any points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature of preprocessing & scaling functions - you MUST use these function signatures in your final submission.\n",
    "\n",
    "def preprocess(dfX):\n",
    "    # insert your code here\n",
    "    dfx=dfX\n",
    "    dfx= pd.get_dummies(dfx, prefix='HeatingFuel_', columns=['HeatingFuel'])\n",
    "    dfx= pd.get_dummies(dfx, prefix='HeatingSystem_', columns=['HeatingSystem'])\n",
    "    dfx= pd.get_dummies(dfx, prefix='SewerSystem_', columns=['SewerSystem'])\n",
    "    dfx[\"HasCentralAC\"] = list(map(lambda x: 1 if x.lower() == 'yes' else 0,dfx[\"HasCentralAC\"]))\n",
    "    dfx[\"AtWaterfront\"] = list(map(lambda x: 1 if x.lower() == 'yes' else 0,dfx[\"AtWaterfront\"]))\n",
    "    columns=[\"LivingArea\",\"LandValue\",\"PropertySize\",\"Age\",\"NeighborhoodHigherEdPct\",\"Bathrooms\",\"Rooms\",\"Fireplaces\",\"HasCentralAC\"]\n",
    "    dfx=dfx[columns]\n",
    "    X=dfx.values\n",
    "    return X #X has to be a two-dimensional numpy array\n",
    "\n",
    "# inputs are both numpy matrices\n",
    "def scale(X_train, X_test):\n",
    "    # insert your code here\n",
    "    mm_scaler = preprocessing.MaxAbsScaler()\n",
    "    mm_scaler.fit(X_train)\n",
    "    X_test_scaled=mm_scaler.transform(X_test)\n",
    "    return X_test_scaled  #X has to be a two-dimensional numpy array as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67627494 0.42705731 0.06491689 ... 1.         0.25       0.        ]\n",
      " [0.18847007 0.01529411 0.07181132 ... 0.33333333 0.         0.        ]\n",
      " [0.26607539 0.07686308 0.0193522  ... 0.5        0.25       0.        ]\n",
      " ...\n",
      " [0.36807095 0.13921715 0.0455647  ... 0.58333333 0.25       1.        ]\n",
      " [0.4345898  0.05098035 0.05385849 ... 0.83333333 0.25       0.        ]\n",
      " [0.2172949  0.08784424 0.02208266 ... 0.58333333 0.25       0.        ]]\n",
      "26638134831.717613\n"
     ]
    }
   ],
   "source": [
    "# testing pipeline, for illustration. Test data will have the same format as the training data. Do NOT edit this cell, \n",
    "# but feel free to copy it to test it yourself.  \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# read in data\n",
    "df_train = pd.read_csv(\"housing_train.csv\")\n",
    "df_test = pd.read_csv(\"housing_test.csv\")\n",
    "\n",
    "# split predictor dataframe from complete data\n",
    "dfX_train = df_train.iloc[:,:-1]\n",
    "dfX_test = df_test.iloc[:,:-1]\n",
    "\n",
    "# preprocess training and test data - preprocessed training data is always needed for scaling\n",
    "X_train = preprocess(dfX_train)\n",
    "X_test = preprocess(dfX_test)\n",
    "\n",
    "# finally, scale your data into a proper format. Note that for scaling the training data,\n",
    "# you should call 'X_train_scaled = scale(X_train, X_train)'\n",
    "X_test_scaled = scale(X_train,X_test)\n",
    "\n",
    "print(X_test_scaled)\n",
    "\n",
    "# load parameter vector resulting from your optimized model\n",
    "beta = np.load(\"beta.npy\")\n",
    "\n",
    "# apply your vector to predict on the test data\n",
    "y_pred = np.dot(X_test_scaled,beta)\n",
    "\n",
    "\n",
    "# get target column from test data and compute MSE\n",
    "y_test = df_test.iloc[:,-1].to_numpy()\n",
    "print(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can illustrate your process of model optimization in the cell(s) below. Feel free to add cells where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Step 1: Data processing\n",
    "#Everything dummies\n",
    "data=pd.read_csv(\"housing_train.csv\")\n",
    "data= pd.get_dummies(data, prefix='HeatingFuel_', columns=['HeatingFuel'])\n",
    "data= pd.get_dummies(data, prefix='HeatingSystem_', columns=['HeatingSystem'])\n",
    "data= pd.get_dummies(data, prefix='SewerSystem_', columns=['SewerSystem'])\n",
    "data[\"HasCentralAC\"] = list(map(lambda x: 1 if x.lower() == 'yes' else 0,data[\"HasCentralAC\"]))\n",
    "data[\"AtWaterfront\"] = list(map(lambda x: 1 if x.lower() == 'yes' else 0,data[\"AtWaterfront\"]))\n",
    "\n",
    "#Delete outliers -- need to see this\n",
    "     #We do two verison with and without outliers and we compare the best  \n",
    "\"\"\"\n",
    "data = data[data[\"PropertySize\"]<4150]\n",
    "data = data[data[\"Age\"]<53]\n",
    "data = data[data[\"LandValue\"]<41000]\n",
    "\n",
    "data = data[data[\"LivingArea\"]<270]\n",
    "data = data[data.Bedrooms.between(2,5)]\n",
    "data = data[data.Price.between(25000,250000)]\n",
    "\"\"\"\n",
    "\n",
    "#Data Correlation\n",
    "matrix = data.corr()\n",
    "interesting_variables = matrix['Price'].sort_values(ascending=False)\n",
    "#print(interesting_variables)\n",
    "\n",
    "#Step 3 Features selection\n",
    "#We did also a Random Forest Regressor to show the most significant features\n",
    "columns=[\"LivingArea\",\"LandValue\",\"PropertySize\",\"Age\",\"NeighborhoodHigherEdPct\",\"Bathrooms\",\"Rooms\",\"Fireplaces\",\"HasCentralAC\",\"Price\"]\n",
    "data=data[columns]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26825164540.088383\n"
     ]
    }
   ],
   "source": [
    "#Here we will compare different models\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression,Lasso, RidgeCV\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "\n",
    "array = data.values\n",
    "y=data[\"Price\"]\n",
    "\n",
    "####deleting the target\n",
    "del(data['Price'])\n",
    "##### the data to learn in a vector --x\n",
    "x=data.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=2)\n",
    "X_test=scale(X_train, X_test)\n",
    "\n",
    "#We did an alpha testing and found out that our best alpha =0.02\n",
    "#We found out that Lasso is the best Regression that minimizes the Data\n",
    "reg = Lasso(alpha=0.02)\n",
    "reg.fit(X_train,y_train)\n",
    "reg.score(X_test, y_test)\n",
    "\n",
    "np.save('beta',reg.coef_)\n",
    "\n",
    "y_pred = np.dot(X_test,reg.coef_)\n",
    "print(mean_squared_error(y_test,y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
